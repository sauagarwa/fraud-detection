# PIPELINE DEFINITION
# Name: 7-get-data-train-upload-deploy
# Inputs:
#    bucket_name: str [Default: 'raw-data']
#    canary: bool [Default: False]
#    data_version: str [Default: '1']
# Outputs:
#    test-model-classification_metrics: system.ClassificationMetrics
#    test-model-metrics: system.Metrics
components:
  comp-deploy-model:
    executorLabel: exec-deploy-model
    inputDefinitions:
      artifacts:
        input_version_file_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        canary:
          parameterType: BOOLEAN
        data_connection_name:
          parameterType: STRING
  comp-get-data:
    executorLabel: exec-get-data
    inputDefinitions:
      parameters:
        bucket_name:
          parameterType: STRING
        data_version:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        data_output_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-test-model:
    executorLabel: exec-test-model
    inputDefinitions:
      artifacts:
        input_model_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        scaler_pkl_input_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        test_data_pkl_input_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        classification_metrics:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
  comp-train-model:
    executorLabel: exec-train-model
    inputDefinitions:
      artifacts:
        data_input_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        model_output_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        scaler_pkl_output_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        test_data_pkl_output_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-upload-model:
    executorLabel: exec-upload-model
    inputDefinitions:
      artifacts:
        input_model_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        version_file_output_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-deploy-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - deploy_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef deploy_model(data_connection_name: str,\n                 canary:\
          \ bool,\n                 input_version_file_path: InputPath()):\n    import\
          \ os, time, subprocess\n    from jinja2 import Template\n    import urllib.request\n\
          \    import os\n\n    variables = {}\n    with open(input_version_file_path)\
          \ as myfile:\n        for line in myfile:\n            name, var = line.partition(\"\
          =\")[::2]\n            variables[name.strip()] = str(var)\n\n    model_name\
          \ = 'fraud' if canary==False or str(canary).lower() == 'false' else f\"\
          fraud-{variables['model_version']}\"\n    template_data = {\"model_version\"\
          : variables['model_version'],\n                    \"storage_key\": data_connection_name,\n\
          \                    'model_name': model_name}\n\n    def download_file(dir,\
          \ filename):\n        url = f\"https://raw.githubusercontent.com/sauagarwa/fraud-detection/main/deployment/{filename}\"\
          \n\n        print(\"starting download...\")\n        urllib.request.urlretrieve(url,\
          \ f\"{dir}/{filename}\")\n        print(\"done\")\n\n        pass\n\n  \
          \  def deploy_template(filename, template_data):\n        print(\"invoking\
          \ template:\" + filename)\n        template = Template(open(filename).read())\n\
          \        rendered_template = template.render(template_data)\n\n        subprocess.run(['oc',\
          \ 'whoami'])\n        ps = subprocess.Popen(['echo', rendered_template],\
          \ stdout=subprocess.PIPE)\n        print(ps.stdout)\n        output = subprocess.check_output(['oc',\
          \ 'apply', '-f', '-'], stdin=ps.stdout)\n        ps.wait()\n        print(f\"\
          Deployed template {filename}. Version: {variables['model_version']}\")\n\
          \n    templates = ['serving-runtime.yaml',\n                 'inference-service.yaml']\n\
          \    for t in templates:\n        dir =  \"/tmp\"\n        download_file(dir,t)\n\
          \        deploy_template(f\"{dir}/{t}\", template_data)\n\n"
        image: quay.io/modh/runtime-images:runtime-cuda-tensorflow-ubi9-python-3.9-2023b-20240301
    exec-get-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_data(data_version: str,\n             bucket_name: str,\n\
          \             data_output_path: OutputPath()):\n    import urllib.request\n\
          \    import os\n\n    endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n\
          \n    print(\"data version = \" + data_version)\n    print(\"endpoint url\
          \ = \" + endpoint_url)\n    print(\"bucket = \" + bucket_name)\n\n    url\
          \ = f\"{endpoint_url}/{bucket_name}/{data_version}/card_transdata.csv\"\n\
          \n    print(\"starting download...\")\n    urllib.request.urlretrieve(url,\
          \ data_output_path)\n    print(\"done\")\n\n"
        image: quay.io/modh/runtime-images:runtime-cuda-tensorflow-ubi9-python-3.9-2023b-20240301
    exec-test-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - test_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'onnxruntime'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef test_model(input_model_path: InputPath(),\n               test_data_pkl_input_path:\
          \ InputPath(),\n               scaler_pkl_input_path: InputPath(),\n   \
          \            metrics: Output[Metrics],\n               classification_metrics:\
          \ Output[ClassificationMetrics]):\n    from sklearn.metrics import confusion_matrix,\
          \ roc_curve\n    import numpy as np\n    import pickle\n    #import seaborn\
          \ as sns\n    #from matplotlib import pyplot as plt\n    import onnxruntime\
          \ as rt\n\n    with open(scaler_pkl_input_path, 'rb') as handle:\n     \
          \   scaler = pickle.load(handle)\n    with open(test_data_pkl_input_path,\
          \ 'rb') as handle:\n        (X_test, y_test) = pickle.load(handle)\n\n \
          \   # Create an ONNX inference runtime session and predict values for all\
          \ test inputs:\n    sess = rt.InferenceSession(input_model_path, providers=rt.get_available_providers())\n\
          \    input_name = sess.get_inputs()[0].name\n    output_name = sess.get_outputs()[0].name\n\
          \    y_pred_temp = sess.run([output_name], {input_name: scaler.transform(X_test.values).astype(np.float32)})\
          \ \n    y_pred_temp = np.asarray(np.squeeze(y_pred_temp[0]))\n    threshold\
          \ = 0.95\n    y_pred = np.where(y_pred_temp > threshold, 1,0)\n\n    accuracy\
          \ = np.sum(np.asarray(y_test) == y_pred) / len(y_pred)\n    print(\"Accuracy:\
          \ \" + str(accuracy))\n\n    c_matrix = confusion_matrix(np.asarray(y_test),y_pred)\n\
          \n    classification_metrics.log_confusion_matrix(\n        [\"Non-Fraud\"\
          , \"Fraud\"],\n        c_matrix.tolist(),  # .tolist() to convert np array\
          \ to list.\n    )\n\n    # fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n\
          \    # classification_metrics.log_roc_curve(fpr,tpr,thresholds)\n\n    metrics.log_metric('accuracy',\
          \ (accuracy*100.0))\n\n"
        image: quay.io/modh/runtime-images:runtime-cuda-tensorflow-ubi9-python-3.9-2023b-20240301
    exec-train-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'tf2onnx' 'seaborn'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_model(data_input_path: InputPath(), \n                test_data_pkl_output_path:\
          \ OutputPath(),\n                scaler_pkl_output_path: OutputPath(),\n\
          \                model_output_path: OutputPath()):\n    import numpy as\
          \ np\n    import pandas as pd\n    from keras.models import Sequential\n\
          \    from keras.layers import Dense, Dropout, BatchNormalization, Activation\n\
          \    from sklearn.model_selection import train_test_split, cross_val_predict\n\
          \    from sklearn.preprocessing import StandardScaler\n    from sklearn.utils\
          \ import class_weight\n    import tf2onnx\n    import onnx\n    import pickle\n\
          \    from pathlib import Path\n\n    # Load the CSV data which we will use\
          \ to train the model.\n    # It contains the following fields:\n    #  \
          \ distancefromhome - The distance from home where the transaction happened.\n\
          \    #   distancefromlast_transaction - The distance from last transaction\
          \ happened.\n    #   ratiotomedianpurchaseprice - Ratio of purchased price\
          \ compared to median purchase price.\n    #   repeat_retailer - If it's\
          \ from a retailer that already has been purchased from before.\n    #  \
          \ used_chip - If the (credit card) chip was used.\n    #   usedpinnumber\
          \ - If the PIN number was used.\n    #   online_order - If it was an online\
          \ order.\n    #   fraud - If the transaction is fraudulent.\n    Data =\
          \ pd.read_csv(data_input_path)\n\n    # Set the input (X) and output (Y)\
          \ data.\n    # The only output data we have is if it's fraudulent or not,\
          \ and all other fields go as inputs to the model.\n\n    X = Data.drop(columns\
          \ = ['repeat_retailer','distance_from_home', 'fraud'])\n    y = Data['fraud']\n\
          \n    # Split the data into training and testing sets so we have something\
          \ to test the trained model with.\n\n    # X_train, X_test, y_train, y_test\
          \ = train_test_split(X,y, test_size = 0.2, stratify = y)\n    X_train, X_test,\
          \ y_train, y_test = train_test_split(X,y, test_size = 0.2, shuffle = False)\n\
          \n    X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,\
          \ test_size = 0.2, stratify = y_train)\n\n    # Scale the data to remove\
          \ mean and have unit variance. This means that the data will be between\
          \ -1 and 1, which makes it a lot easier for the model to learn than random\
          \ potentially large values.\n    # It is important to only fit the scaler\
          \ to the training data, otherwise you are leaking information about the\
          \ global distribution of variables (which is influenced by the test set)\
          \ into the training set.\n\n    scaler = StandardScaler()\n\n    X_train\
          \ = scaler.fit_transform(X_train.values)\n\n    #Path(\"artifact\").mkdir(parents=True,\
          \ exist_ok=True)\n    with open(test_data_pkl_output_path, \"wb\") as handle:\n\
          \        pickle.dump((X_test, y_test), handle)\n    with open(scaler_pkl_output_path,\
          \ \"wb\") as handle:\n        pickle.dump(scaler, handle)\n\n    # Since\
          \ the dataset is unbalanced (it has many more non-fraud transactions than\
          \ fraudulent ones), we set a class weight to weight the few fraudulent transactions\
          \ higher than the many non-fraud transactions.\n\n    class_weights = class_weight.compute_class_weight('balanced',classes\
          \ = np.unique(y_train),y = y_train)\n    class_weights = {i : class_weights[i]\
          \ for i in range(len(class_weights))}\n\n\n    # Build the model, the model\
          \ we build here is a simple fully connected deep neural network, containing\
          \ 3 hidden layers and one output layer.\n\n    model = Sequential()\n  \
          \  model.add(Dense(32, activation = 'relu', input_dim = len(X.columns)))\n\
          \    model.add(Dropout(0.2))\n    model.add(Dense(32))\n    model.add(BatchNormalization())\n\
          \    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(32))\n\
          \    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n\
          \    model.add(Dropout(0.2))\n    model.add(Dense(1, activation = 'sigmoid'))\n\
          \    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n\
          \    model.summary()\n\n\n    # Train the model and get performance\n\n\
          \    epochs = 2\n    history = model.fit(X_train, y_train, epochs=epochs,\
          \ \\\n                        validation_data=(scaler.transform(X_val.values),y_val),\
          \ \\\n                        verbose = True, class_weight = class_weights)\n\
          \n    # Save the model as ONNX for easy use of ModelMesh\n\n    model_proto,\
          \ _ = tf2onnx.convert.from_keras(model)\n    print(model_output_path)\n\
          \    onnx.save(model_proto, model_output_path)\n\n"
        image: quay.io/modh/runtime-images:runtime-cuda-tensorflow-ubi9-python-3.9-2023b-20240301
    exec-upload-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'boto3' 'botocore'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef upload_model(input_model_path: InputPath(), version_file_output_path:\
          \ OutputPath()):\n\n    # Save the model as ONNX for easy use of ModelMesh\n\
          \    import os\n    import boto3\n    import botocore\n    from datetime\
          \ import datetime\n\n    aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n\
          \    aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n \
          \   endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n    region_name =\
          \ os.environ.get('AWS_DEFAULT_REGION')\n    bucket_name = os.environ.get('AWS_S3_BUCKET')\n\
          \n    model_version = datetime.now().strftime(\"%y-%m-%d-%H%M%S\")\n   \
          \ print(\"model version = \" + model_version)\n\n    object_name = 'model-'\
          \ + model_version +'/fraud/1/model.onnx'\n    print(\"object name = \" +\
          \ object_name)\n\n    # Set up the S3 client\n    s3 = boto3.client('s3',\n\
          \                      endpoint_url=endpoint_url,\n                    \
          \  aws_access_key_id=aws_access_key_id,\n                      aws_secret_access_key=aws_secret_access_key)\n\
          \n\n    # upload the model to the registry\n    s3.upload_file(input_model_path,\
          \ bucket_name, object_name)\n\n    print(\"version output path = \" + version_file_output_path)\n\
          \    # store the model version in a file temporarily for the next pipeline\
          \ job\n    with open(version_file_output_path, \"w\") as text_file:\n  \
          \      text_file.write('model_version='+model_version)\n\n"
        image: quay.io/modh/runtime-images:runtime-cuda-tensorflow-ubi9-python-3.9-2023b-20240301
pipelineInfo:
  name: 7-get-data-train-upload-deploy
root:
  dag:
    outputs:
      artifacts:
        test-model-classification_metrics:
          artifactSelectors:
          - outputArtifactKey: classification_metrics
            producerSubtask: test-model
        test-model-metrics:
          artifactSelectors:
          - outputArtifactKey: metrics
            producerSubtask: test-model
    tasks:
      deploy-model:
        cachingOptions: {}
        componentRef:
          name: comp-deploy-model
        dependentTasks:
        - upload-model
        inputs:
          artifacts:
            input_version_file_path:
              taskOutputArtifact:
                outputArtifactKey: version_file_output_path
                producerTask: upload-model
          parameters:
            canary:
              componentInputParameter: canary
            data_connection_name:
              runtimeValue:
                constant: aws-connection-fraud-detection
        taskInfo:
          name: deploy-model
      get-data:
        cachingOptions: {}
        componentRef:
          name: comp-get-data
        inputs:
          parameters:
            bucket_name:
              componentInputParameter: bucket_name
            data_version:
              componentInputParameter: data_version
        taskInfo:
          name: get-data
      test-model:
        cachingOptions: {}
        componentRef:
          name: comp-test-model
        dependentTasks:
        - train-model
        inputs:
          artifacts:
            input_model_path:
              taskOutputArtifact:
                outputArtifactKey: model_output_path
                producerTask: train-model
            scaler_pkl_input_path:
              taskOutputArtifact:
                outputArtifactKey: scaler_pkl_output_path
                producerTask: train-model
            test_data_pkl_input_path:
              taskOutputArtifact:
                outputArtifactKey: test_data_pkl_output_path
                producerTask: train-model
        taskInfo:
          name: test-model
      train-model:
        cachingOptions: {}
        componentRef:
          name: comp-train-model
        dependentTasks:
        - get-data
        inputs:
          artifacts:
            data_input_path:
              taskOutputArtifact:
                outputArtifactKey: data_output_path
                producerTask: get-data
        taskInfo:
          name: train-model
      upload-model:
        cachingOptions: {}
        componentRef:
          name: comp-upload-model
        dependentTasks:
        - test-model
        - train-model
        inputs:
          artifacts:
            input_model_path:
              taskOutputArtifact:
                outputArtifactKey: model_output_path
                producerTask: train-model
        taskInfo:
          name: upload-model
  inputDefinitions:
    parameters:
      bucket_name:
        defaultValue: raw-data
        isOptional: true
        parameterType: STRING
      canary:
        defaultValue: false
        isOptional: true
        parameterType: BOOLEAN
      data_version:
        defaultValue: '1'
        isOptional: true
        parameterType: STRING
  outputDefinitions:
    artifacts:
      test-model-classification_metrics:
        artifactType:
          schemaTitle: system.ClassificationMetrics
          schemaVersion: 0.0.1
      test-model-metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.9.0
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-deploy-model:
          secretAsEnv:
          - keyToEnv:
            - envVar: AWS_ACCESS_KEY_ID
              secretKey: AWS_ACCESS_KEY_ID
            - envVar: AWS_SECRET_ACCESS_KEY
              secretKey: AWS_SECRET_ACCESS_KEY
            - envVar: AWS_DEFAULT_REGION
              secretKey: AWS_DEFAULT_REGION
            - envVar: AWS_S3_BUCKET
              secretKey: AWS_S3_BUCKET
            - envVar: AWS_S3_ENDPOINT
              secretKey: AWS_S3_ENDPOINT
            secretName: aws-connection-fraud-detection
        exec-get-data:
          secretAsEnv:
          - keyToEnv:
            - envVar: AWS_ACCESS_KEY_ID
              secretKey: AWS_ACCESS_KEY_ID
            - envVar: AWS_SECRET_ACCESS_KEY
              secretKey: AWS_SECRET_ACCESS_KEY
            - envVar: AWS_DEFAULT_REGION
              secretKey: AWS_DEFAULT_REGION
            - envVar: AWS_S3_BUCKET
              secretKey: AWS_S3_BUCKET
            - envVar: AWS_S3_ENDPOINT
              secretKey: AWS_S3_ENDPOINT
            secretName: aws-connection-fraud-detection
        exec-upload-model:
          secretAsEnv:
          - keyToEnv:
            - envVar: AWS_ACCESS_KEY_ID
              secretKey: AWS_ACCESS_KEY_ID
            - envVar: AWS_SECRET_ACCESS_KEY
              secretKey: AWS_SECRET_ACCESS_KEY
            - envVar: AWS_DEFAULT_REGION
              secretKey: AWS_DEFAULT_REGION
            - envVar: AWS_S3_BUCKET
              secretKey: AWS_S3_BUCKET
            - envVar: AWS_S3_ENDPOINT
              secretKey: AWS_S3_ENDPOINT
            secretName: aws-connection-fraud-detection
